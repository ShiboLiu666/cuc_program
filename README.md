# 实验一：利用SVD简化数据
## 1.SVD原理简介
SVD分解能够将任意矩阵着矩阵（m*n）分解成三个矩阵U（m*m）、Σ（m*n）、VT（n*n），如下：Datamxn = Umxm∑mxnVTnxn。Σ是一个对角矩阵，其对角元素是从大到小排列的，它们对应了Data矩阵的奇异值。当对矩阵进行奇异值分解时，发现大部分奇异值为零，即说明矩阵的信息大部分集中在少量奇异值上，因此可以通过矩阵的近似低维表示来代替原先矩阵。通过选取前K个奇异值来近似表示矩阵奇异值的个数选择，可以采用奇异值平方和的累加占比来决定，即选择前k个奇异值的平方和占全部奇异值的平方和的比值大于一定阈值。
## 2.利用python实现SVD
## 3.基于SVD的推荐引擎
## 4.基于SVD的图像压缩

# 实验二：梯度下降法
## 1.实验原理
梯度的意义：在单变量的函数中，梯度就是函数的微分，即函数在某个点的切线的斜率；在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在某个点处上升最快的方向；那么，梯度的反方向就是函数在某个点处下降最快的方向，如果想要求最低点，一直沿着梯度的反方向走即可。
算法思想：梯度下降是一种非常通用的优化算法，能够为大范围的问题找到最优解。梯度下降的中心思想就是迭代地调整参数从而使损失函数最小化。通过测量参数向量 θ 相关的损失函数的局部梯度，并不断沿着降低梯度的方向调整，直到梯度降为 0 ，达到最小值。
## 2.实验过程
2.1 定义数据集和学习率
2.2 定义代价函数和代价函数的梯度
2.3 梯度下降迭代计算
2.4 输出结果
